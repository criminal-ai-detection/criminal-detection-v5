{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch import nn\n",
    "# from torch.nn import functional as F\n",
    "\n",
    "# from typing import Any, Dict, List, Tuple, Union\n",
    "\n",
    "\n",
    "# class CustomSam(nn.Module):\n",
    "#     \"\"\"\n",
    "#     this only uses the sam methods necessary for TinyVit encoder\n",
    "#     \"\"\"\n",
    "#     mask_threshold: float = 0.0\n",
    "#     image_format: str = \"RGB\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         # image_encoder: Union[ImageEncoderViT, TinyViT], #usaremos TinyVit en este caso\n",
    "#         image_encoder,\n",
    "#         pixel_mean: List[float] = [123.675, 116.28, 103.53],\n",
    "#         pixel_std: List[float] = [58.395, 57.12, 57.375],\n",
    "#     ) -> None:\n",
    "       \n",
    "#         super().__init__()\n",
    "#         self.image_encoder = image_encoder\n",
    "#         self.register_buffer(\"pixel_mean\", torch.Tensor(pixel_mean).view(-1, 1, 1), False)\n",
    "#         self.register_buffer(\"pixel_std\", torch.Tensor(pixel_std).view(-1, 1, 1), False)\n",
    "\n",
    "#     @property\n",
    "#     def device(self) -> Any:\n",
    "#         return self.pixel_mean.device\n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def forward(\n",
    "#         self,\n",
    "#         batched_input: List[Dict[str, Any]],\n",
    "    # ) -> List[Dict[str, torch.Tensor]]:\n",
    "    #     # input_images = torch.stack([self.preprocess(x[\"image\"]) for x in batched_input], dim=0)\n",
    "    #     input_images = self.preprocess(batched_input)\n",
    "    #     image_embeddings = self.image_encoder(input_images)\n",
    "\n",
    "    #     # outputs = []\n",
    "    #     return image_embeddings\n",
    "\n",
    "   \n",
    "    # def preprocess(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    #     \"\"\"Normalize pixel values and pad to a square input.\"\"\"\n",
    "    #     # Normalize colors\n",
    "    #     x = (x - self.pixel_mean) / self.pixel_std\n",
    "\n",
    "    #     # Pad\n",
    "    #     h, w = x.shape[-2:]\n",
    "    #     padh = self.image_encoder.img_size - h\n",
    "    #     padw = self.image_encoder.img_size - w\n",
    "    #     x = F.pad(x, (0, padw, 0, padh))\n",
    "    #     return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\mobile_sam\\modeling\\tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_5m_224 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "c:\\Python311\\Lib\\site-packages\\mobile_sam\\modeling\\tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_11m_224 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "c:\\Python311\\Lib\\site-packages\\mobile_sam\\modeling\\tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_21m_224 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "c:\\Python311\\Lib\\site-packages\\mobile_sam\\modeling\\tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_21m_384 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "c:\\Python311\\Lib\\site-packages\\mobile_sam\\modeling\\tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_21m_512 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n"
     ]
    }
   ],
   "source": [
    "from mobile_sam.modeling.image_encoder import ImageEncoderViT\n",
    "from custom_sam import CustomSam\n",
    "import torch\n",
    "\n",
    "# Crear una instancia del modelo\n",
    "model = ImageEncoderViT() #img_size= lo que quieras\n",
    "\n",
    "sam_encoder = CustomSam(image_encoder=model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_encoder.to(device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "x = torch.randn(1,3, 1024, 1024).to(device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# result = sam_encoder(x)\n",
    "with torch.autocast(device_type=\"cuda\"):\n",
    "    result = sam_encoder(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.1283, -0.7257, -0.7272,  ..., -0.7355, -0.7234, -1.9037],\n",
       "          [ 0.7464, -0.5251, -0.5346,  ..., -0.5265, -0.5186, -2.2405],\n",
       "          [ 0.7491, -0.5219, -0.5249,  ..., -0.5244, -0.5242, -2.2300],\n",
       "          ...,\n",
       "          [ 0.7567, -0.5222, -0.5201,  ..., -0.5275, -0.5175, -2.2296],\n",
       "          [ 0.7590, -0.5268, -0.5316,  ..., -0.5312, -0.5410, -2.2430],\n",
       "          [ 0.9944, -0.5863, -0.5860,  ..., -0.5794, -0.5979, -2.1611]],\n",
       "\n",
       "         [[-0.1232, -0.0586, -0.0593,  ..., -0.0579, -0.0528,  0.6406],\n",
       "          [ 0.0378,  0.0223,  0.0131,  ...,  0.0195,  0.0190, -0.0229],\n",
       "          [ 0.0444,  0.0345,  0.0139,  ...,  0.0211,  0.0171, -0.0253],\n",
       "          ...,\n",
       "          [ 0.0368,  0.0235,  0.0254,  ...,  0.0211,  0.0306, -0.0282],\n",
       "          [ 0.0419,  0.0196,  0.0272,  ...,  0.0128,  0.0155, -0.0388],\n",
       "          [-0.2152,  0.0215,  0.0176,  ...,  0.0172,  0.0242, -0.7722]],\n",
       "\n",
       "         [[-0.0285,  0.7279,  0.7368,  ...,  0.7280,  0.7416,  1.4461],\n",
       "          [ 0.0907,  0.2290,  0.2392,  ...,  0.2341,  0.2403,  0.9926],\n",
       "          [ 0.0973,  0.2387,  0.2406,  ...,  0.2378,  0.2459,  0.9847],\n",
       "          ...,\n",
       "          [ 0.1092,  0.2347,  0.2482,  ...,  0.2456,  0.2486,  0.9790],\n",
       "          [ 0.1072,  0.2459,  0.2421,  ...,  0.2362,  0.2280,  0.9727],\n",
       "          [ 0.3269, -0.3523, -0.3477,  ..., -0.3403, -0.3445,  0.3587]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.2382,  0.1039,  0.1098,  ...,  0.1119,  0.1106,  0.9338],\n",
       "          [-0.6962,  0.3937,  0.4040,  ...,  0.3993,  0.4101,  0.8891],\n",
       "          [-0.6919,  0.3990,  0.4078,  ...,  0.3978,  0.3891,  0.8839],\n",
       "          ...,\n",
       "          [-0.7022,  0.3948,  0.3861,  ...,  0.3975,  0.3925,  0.8947],\n",
       "          [-0.6949,  0.3899,  0.3980,  ...,  0.3973,  0.4057,  0.8950],\n",
       "          [ 0.2220,  1.3086,  1.2889,  ...,  1.2958,  1.2997,  1.0155]],\n",
       "\n",
       "         [[ 0.2891,  0.4821,  0.4693,  ...,  0.4879,  0.4734,  1.1213],\n",
       "          [ 0.5002,  0.5981,  0.5920,  ...,  0.6034,  0.5977,  0.9565],\n",
       "          [ 0.5008,  0.5928,  0.6009,  ...,  0.5995,  0.5989,  0.9551],\n",
       "          ...,\n",
       "          [ 0.4977,  0.5960,  0.6058,  ...,  0.5953,  0.6082,  0.9484],\n",
       "          [ 0.4959,  0.6036,  0.5929,  ...,  0.5890,  0.5989,  0.9514],\n",
       "          [ 0.6634,  0.6966,  0.7023,  ...,  0.6913,  0.6895,  0.8859]],\n",
       "\n",
       "         [[-0.3062, -0.1934, -0.1942,  ..., -0.1847, -0.1834, -0.3957],\n",
       "          [-0.1234, -0.5532, -0.5609,  ..., -0.5650, -0.5523, -0.3457],\n",
       "          [-0.1298, -0.5608, -0.5517,  ..., -0.5451, -0.5518, -0.3297],\n",
       "          ...,\n",
       "          [-0.1296, -0.5554, -0.5500,  ..., -0.5486, -0.5452, -0.3439],\n",
       "          [-0.1283, -0.5622, -0.5571,  ..., -0.5480, -0.5531, -0.3460],\n",
       "          [ 0.0579, -0.9900, -0.9846,  ..., -0.9938, -0.9858, -0.5403]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 64, 64])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ImageEncoderViT(img_size=200) #img_size= lo que quieras (altura de la imagen)\n",
    "\n",
    "# torch.compile(model)\n",
    "\n",
    "# sam_encoder = CustomSam(image_encoder=model)\n",
    "\n",
    "# sam_encoder\n",
    "\n",
    "# x = torch.randn(1,3, 1024, 200)\n",
    "\n",
    "# # with torch.autocast(device_type=\"cpu\"):\n",
    "# result = sam_encoder(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.2853, -0.6384, -0.6394,  ..., -0.6364, -0.6322, -0.5102],\n",
       "          [-0.0857, -0.6047, -0.5946,  ..., -0.6021, -0.5966, -0.3930],\n",
       "          [-0.0839, -0.6091, -0.6016,  ..., -0.6075, -0.5982, -0.3947],\n",
       "          ...,\n",
       "          [-0.0837, -0.6074, -0.6070,  ..., -0.5963, -0.6048, -0.3942],\n",
       "          [-0.0832, -0.5981, -0.6057,  ..., -0.6000, -0.6040, -0.3995],\n",
       "          [ 0.8127, -0.0631, -0.0732,  ..., -0.0657, -0.0714, -0.1949]],\n",
       "\n",
       "         [[-0.5471,  0.1058,  0.1042,  ...,  0.1055,  0.1117,  0.3792],\n",
       "          [-1.0217, -0.1610, -0.1777,  ..., -0.1599, -0.1722,  0.3407],\n",
       "          [-1.0264, -0.1744, -0.1762,  ..., -0.1764, -0.1718,  0.3350],\n",
       "          ...,\n",
       "          [-1.0301, -0.1684, -0.1625,  ..., -0.1703, -0.1774,  0.3458],\n",
       "          [-1.0323, -0.1688, -0.1721,  ..., -0.1660, -0.1680,  0.3324],\n",
       "          [-0.0121,  0.7998,  0.7840,  ...,  0.7964,  0.7935,  0.7871]],\n",
       "\n",
       "         [[-1.4720, -1.3422, -1.3463,  ..., -1.3478, -1.3447, -1.1416],\n",
       "          [-0.6710, -0.8197, -0.8211,  ..., -0.8091, -0.8223, -0.7272],\n",
       "          [-0.6588, -0.8203, -0.8064,  ..., -0.8158, -0.8053, -0.7129],\n",
       "          ...,\n",
       "          [-0.6690, -0.8198, -0.8191,  ..., -0.8260, -0.8110, -0.7241],\n",
       "          [-0.6694, -0.8202, -0.8095,  ..., -0.8107, -0.8313, -0.7152],\n",
       "          [-0.4394, -0.6604, -0.6627,  ..., -0.6817, -0.6699, -0.4206]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.0237, -0.6721, -0.6619,  ..., -0.6680, -0.6584, -0.7137],\n",
       "          [ 0.1193, -0.4928, -0.4868,  ..., -0.4713, -0.4856, -0.4374],\n",
       "          [ 0.1165, -0.4909, -0.4839,  ..., -0.4803, -0.4723, -0.4350],\n",
       "          ...,\n",
       "          [ 0.1335, -0.4803, -0.4887,  ..., -0.4795, -0.4871, -0.4366],\n",
       "          [ 0.1259, -0.4934, -0.4844,  ..., -0.4773, -0.4768, -0.4347],\n",
       "          [-0.5736, -0.8673, -0.8553,  ..., -0.8688, -0.8615, -0.3524]],\n",
       "\n",
       "         [[ 1.0523,  1.3486,  1.3452,  ...,  1.3403,  1.3468,  1.8250],\n",
       "          [ 0.9122,  1.1417,  1.1287,  ...,  1.1203,  1.1247,  1.2197],\n",
       "          [ 0.9047,  1.1280,  1.1374,  ...,  1.1284,  1.1296,  1.2169],\n",
       "          ...,\n",
       "          [ 0.9133,  1.1255,  1.1327,  ...,  1.1332,  1.1312,  1.2269],\n",
       "          [ 0.9128,  1.1267,  1.1338,  ...,  1.1396,  1.1214,  1.2258],\n",
       "          [ 0.5111,  0.2157,  0.2142,  ...,  0.2195,  0.2095,  0.1188]],\n",
       "\n",
       "         [[-0.1568, -0.5084, -0.5215,  ..., -0.5159, -0.5132, -0.3463],\n",
       "          [-0.1329, -0.6755, -0.6780,  ..., -0.6698, -0.6837, -0.8429],\n",
       "          [-0.1311, -0.6858, -0.6792,  ..., -0.6715, -0.6835, -0.8258],\n",
       "          ...,\n",
       "          [-0.1310, -0.6651, -0.6729,  ..., -0.6775, -0.6757, -0.8414],\n",
       "          [-0.1314, -0.6771, -0.6724,  ..., -0.6790, -0.6798, -0.8353],\n",
       "          [ 0.4381, -0.2706, -0.2576,  ..., -0.2602, -0.2586, -0.6961]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
